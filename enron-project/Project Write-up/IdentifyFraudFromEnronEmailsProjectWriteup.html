<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c7{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c4{background-color:#ffffff;color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c3{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c0{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Times New Roman";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c16{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;text-align:left}.c9{border-spacing:0;border-collapse:collapse;margin-right:auto}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c8{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c14{background-color:#ffffff;font-style:italic}.c10{background-color:#ffffff;font-weight:700}.c6{height:0pt}.c17{background-color:#ffffff}.c11{background-color:#ffff00}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c8"><p class="c16 title" id="h.610w5tg9c8u6"><span class="c15">Identify Fraud From Enron Emails Project Write-up</span></p><p class="c5"><span class="c13">By: Christian Guzman&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 10/2017</span></p><p class="c1"><span class="c12"></span></p><p class="c5"><span class="c10">1. </span><span class="c4">Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c17">The goal of this project was to create a persons of interest identifier within the &nbsp;data from the enron corpus of emails and financial information. The data set had over 145 employees serving as data points, each with 21 features consisting of email and financial data. The label was an indicator variable poi, for person of interest. A person of interest being an individual who was </span><span class="c14">&ldquo;indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.&rdquo;</span><span class="c0">&nbsp;With this data set it is possible to classify whether or not an employee was a person of interest. 18 of the 146 data points were indicated to be POIs. This means that since only 12% of employees are a person of interest, if a model guessed false for all data points the model would have 88% accuracy! For this reason accuracy metric will not be useful for this analysis.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 453.00px; height: 272.00px;"><img alt="" src="images/image8.png" style="width: 453.00px; height: 272.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c0">In general for outliers with 146 data points I wanted to be careful when removing outliers and to ensure to not get rid of most of the data. I expected persons of interest to have unique attributes, so outliers could prove useful, as such I only wanted to remove data points that did not fit with the others. Looking at the distribution of salary it is very easy to point out an outlier with a salary of 25 million dollars, this was excessively high and when looking at the source of this outlier, the data point was not an employee, but the total salary of all employees. Another outlier that was removed was a data point with the name, &ldquo;THE TRAVEL AGENCY IN THE PARK.&rdquo; This was obviously not an employee. Using the IQR method for detecting outliers I noticed that there were negative values for features that did not make sense to be negative, such as bonus, and income. From my research stock value can also not be negative, so I elected to transform these negative values into positive.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c4">2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.</span></p><p class="c1"><span class="c4"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 322.00px; height: 374.00px;"><img alt="" src="images/image7.png" style="width: 322.00px; height: 374.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 311.00px; height: 105.00px;"><img alt="" src="images/image6.png" style="width: 311.00px; height: 105.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c0">From the above figures it can be seen that 98% of the feature, &lsquo;loan_advances&rsquo; are null values. There also does not seem to be any discerning relation between persons of interest and this feature and so I removed it from further analysis.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">Initially I had 11 features that I hand picked, they included salary, bonus, stock and email data. I wanted to leave most of the feature selection to the SelectKBest algorithm, and also further reduced using principle component analysis for any classifiers. These two steps are included in all the pipelines. I only had to feature scale for the SVM algorithm, but not for the other classifiers.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">The top features were mostly stock and income features with the only email related feature being the number email from a person of interest and number of emails shared receipt with a person of interest.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 356.00px; height: 256.00px;"><img alt="" src="images/image4.png" style="width: 356.00px; height: 256.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c0">I created an extra feature called &ldquo;poi_email_ratio&rdquo; which is basically the ratio of emails to and from a person of interest. This feature was created by dividing total emails to and from a person of interest, and the total number of emails in the inbox. The below table are feature importances after appending my new feature.</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 407.00px; height: 260.00px;"><img alt="" src="images/image1.png" style="width: 407.00px; height: 260.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">The same features were still the top most important, but my new feature &ldquo;poi_email_ratio&rdquo; was selected among the 8 best features.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c4">3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">For my final and tuned classifier I decided to use the Decision Tree algorithm. Decision trees can be prone to overfitting but with only two features from the PCA the algorithm is less complex. Before this decision I also experimented with, Naive Bayes, Support Vector Machine, Random Forest classifiers and Logistic Regression.</span></p><p class="c1"><span class="c0"></span></p><p class="c1"><span class="c0"></span></p><a id="t.47d66ae71c4ba3f63c02d6913c3d5aaa93b1ab44"></a><a id="t.0"></a><table class="c9"><tbody><tr class="c6"><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c4">Algorithm</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c4">Precision</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c4">Recall</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c4">F1 Score</span></p></td></tr><tr class="c6"><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">Decision Tree</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.38066</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.34450</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.36168</span></p></td></tr><tr class="c6"><td class="c7 c11" colspan="1" rowspan="1"><p class="c2"><span class="c0">Naive Bayes</span></p></td><td class="c7 c11" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.46082</span></p></td><td class="c7 c11" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.34400</span></p></td><td class="c7 c11" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.39393</span></p></td></tr><tr class="c6"><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">Support Vector Machine</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.48387</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.00750</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.01477</span></p></td></tr><tr class="c6"><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">Random Forest</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.36652</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.12700</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.18864</span></p></td></tr><tr class="c6"><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">Logistic Regression</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.22740</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.46400</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c2"><span class="c0">0.30521</span></p></td></tr></tbody></table><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">The Naive Bayes algorithm had the highest precision, recall and thus F1 score out of all algorithms. The second best performer was the Decision Tree algorithm, I decided to tune this algorithm for my final classifier. Support Vector Machine performed the worst in my experimentations and had low Recall scores with the highest score I&rsquo;ve observed, by adjusting the C parameter, was approximately 0.1. It was also the slowest algorithm.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c4">4. What does it mean to tune the parameters of an algorithm, and what can happen if you don&rsquo;t do this well? &nbsp;How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).</span></p><p class="c1"><span class="c4"></span></p><p class="c5"><span class="c0">Tuning the parameters of an algorithm means to adjust how the algorithm will classify features for the purpose of improving an evaluation metric such as accuracy of predictions or precision, and recall score. If an algorithm is not tuned correctly then the algorithm could overfit to the data unnecessarily, and be a high-variance model that performs well on training data, but not so much on test, or new data. Of course the opposite of overfitting can occur with bad tuning, creating an oversimplified model. To tune the parameters of an algorithm means to balance the Bias-Variance trade-off.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">For the Decision Tree algorithm, I wanted to focus on tuning the parameters for number of features selected by KBest, number of Principal component features, and the minimum number of samples split. I used GridSearchCV algorithm to automatically score the algorithm with different parameters of my choosing. By tuning feature selection, and reduction parameters the algorithm can be scored with different levels of complexity. The minimum number of sample splitting tuning is a way to test decision boundaries of varying complexity.</span></p><p class="c1"><span class="c4"></span></p><p class="c5"><span class="c10">Final 7 features chosen before principal component analysis:</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 382.00px; height: 233.00px;"><img alt="" src="images/image3.png" style="width: 382.00px; height: 233.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c4">5. What is validation, and what&rsquo;s a classic mistake you can make if you do it wrong? How did you validate your analysis?</span></p><p class="c1"><span class="c4"></span></p><p class="c5"><span class="c0">Validation is the process of evaluating the results of a model on portion of the data set not used in training the model. This is done to avoid overfitting on the data set. Avoiding overfitting on the data is important because it allows the model to be tested on new data. It is a good way to see the performance of the model on an independent data set. For my analysis I split off 30% of data points to be test data, and the remaining 70% of the data was split again using the GridSearchCV algorithm which performs its own cross-validation on the data. The best estimator was then used to make predictions on the test data and the performance was evaluated.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c4">6. Give at least 2 evaluation metrics and your average performance for each of them. &nbsp;Explain an interpretation of your metrics that says something human-understandable about your algorithm&rsquo;s performance.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">For evaluating the performance of my algorithm, I looked at f1, precision, and recall scores. The f1 score being the weighted average between precision and recall score is a good metric to observe how well the model performed. The precision scores tells us how good the algorithm is at identifying a person of interest and minimizing false positives i.e. incorrect identifications. The recall score tells us how well the algorithm is identifying persons of interest given that the employee is one. Below is the result of predicting with the test data before the splitting with GridSearchCV. </span></p><p class="c1"><span class="c3"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 507.00px; height: 138.00px;"><img alt="" src="images/image5.png" style="width: 507.00px; height: 138.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">The algorithm was able to identify persons of interest with .2 precision and 0.17 recall, with an f1 score of 0.18.</span></p><p class="c5"><span class="c4">Tester.py results</span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 647.50px; height: 51.00px;"><img alt="" src="images/image2.png" style="width: 647.50px; height: 51.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c0">With 14,000 predictions and the randomized validation, sampling with Stratified Shuffle Split by GridSearchCV. The algorithm identifies with 0.36 precision, 0.32 recall and an F1 score of 0.34.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c4">Conclusion</span></p><p class="c1"><span class="c4"></span></p><p class="c5"><span class="c0">Out of the 22 features I handpicked 11, and I created the extra feature &ldquo;poi_email_ratio&rdquo; under the assumption that persons of interest should theoretically have a ratio of their emails having any sort of relation from another person of interest. In my experimentations Naive Bayes was the best performing algorithm, but I decided to continue with the second best performing, Decision Tree for further tuning. Using the GridsearchCV for tuning parameters, best number of features was 7 and this did not include my new feature. The 7 features were then reduced into 3 principal components, a good way to reduce complexity of model. After the feature selection and reduction of features, the Decision Tree algorithm was used with the 2 sample minimum for creating splits for classification, and this was my final tuned algorithm.</span></p><p class="c1"><span class="c0"></span></p><p class="c5"><span class="c0">There are potentially many other unknown factors in what makes a person of interest but with all this .3, and precision and recall score was able to be reached with the financial and email data.</span></p></body></html>